{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/russellemergentai/MistralInstruct/blob/main/Langchain_Agent_with_Mistral7B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install langchain\n",
        "!pip install langchain-community\n",
        "!pip install wikipedia\n",
        "!pip install accelerate\n",
        "!pip install bitsandbytes\n",
        "!pip install wikipedia\n",
        "\n",
        "# Suppress warnings (optional)\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import core libraries and dependencies\n",
        "import numexpr as ne\n",
        "import torch\n",
        "from typing import Optional, List, Mapping, Any\n",
        "\n",
        "# Import transformers models and utilities\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from transformers.models.mistral.modeling_mistral import MistralForCausalLM\n",
        "from transformers.models.llama.tokenization_llama_fast import LlamaTokenizerFast\n",
        "\n",
        "# Import LangChain modules and utilities\n",
        "from langchain.tools import WikipediaQueryRun, BaseTool\n",
        "from langchain.agents import Tool\n",
        "from langchain_community.utilities import WikipediaAPIWrapper\n",
        "from langchain.llms.base import LLM\n",
        "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.agents import create_json_chat_agent, AgentExecutor\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# load model and tokenizer\n",
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, quantization_config=quantization_config, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# wrap the LLNM\n",
        "class CustomLLMMistral(LLM):\n",
        "    model: MistralForCausalLM\n",
        "    tokenizer: LlamaTokenizerFast\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"custom\"\n",
        "\n",
        "    def _call(self, prompt: str, stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[CallbackManagerForLLMRun] = None) -> str:\n",
        "\n",
        "        messages = [\n",
        "         {\"role\": \"user\", \"content\": prompt},\n",
        "        ]\n",
        "\n",
        "        encodeds = self.tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
        "        model_inputs = encodeds.to(self.model.device)\n",
        "\n",
        "        generated_ids = self.model.generate(model_inputs, max_new_tokens=512, do_sample=True, pad_token_id=tokenizer.eos_token_id, top_k=4, temperature=0.7)\n",
        "        decoded = self.tokenizer.batch_decode(generated_ids)\n",
        "\n",
        "        output = decoded[0].split(\"[/INST]\")[1].replace(\"</s>\", \"\").strip()\n",
        "\n",
        "        if stop is not None:\n",
        "          for word in stop:\n",
        "            output = output.split(word)[0].strip()\n",
        "\n",
        "        # Mistral 7B sometimes fails to properly close the Markdown Snippets.\n",
        "        # If they are not correctly closed, Langchain will struggle to parse the output.\n",
        "        while not output.endswith(\"```\"):\n",
        "          output += \"`\"\n",
        "\n",
        "        return output\n",
        "\n",
        "    @property\n",
        "    def _identifying_params(self) -> Mapping[str, Any]:\n",
        "        return {\"model\": self.model}\n",
        "\n",
        "\n",
        "llm = CustomLLMMistral(model=model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "h5ARqsLzLM_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tools"
      ],
      "metadata": {
        "id": "1-fsPHA4hfBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=2500))\n",
        "\n",
        "wikipedia_tool = Tool(\n",
        "    name=\"wikipedia\",\n",
        "    description=\"Never search for more than one concept at a single step. If you need to compare two concepts, search for each one individually. Syntax: string with a simple concept\",\n",
        "    func=wikipedia.run\n",
        ")\n",
        "\n",
        "class Calculator(BaseTool):\n",
        "    name: str = \"calculator\"\n",
        "    description: str = \"Use this tool for math operations. It requires numexpr syntax. Use it always you need to solve any math operation. Be sure syntax is correct.\"\n",
        "\n",
        "    def _run(self, expression: str):\n",
        "      try:\n",
        "        return ne.evaluate(expression).item()\n",
        "      except Exception:\n",
        "        return \"This is not a numexpr valid syntax. Try a different syntax.\"\n",
        "\n",
        "    def _arun(self, radius: int):\n",
        "        raise NotImplementedError(\"This tool does not support async\")\n",
        "\n",
        "calculator_tool = Calculator()\n",
        "\n",
        "tools = [wikipedia_tool, calculator_tool]\n"
      ],
      "metadata": {
        "id": "8qXMWtcPaNMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt"
      ],
      "metadata": {
        "id": "ieNdbNj9hakQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system=\"\"\"\n",
        "You are designed to solve tasks. Each task requires multiple steps that are represented by a markdown code snippet of a json blob.\n",
        "The json structure should contain the following keys:\n",
        "thought -> your thoughts\n",
        "action -> name of a tool\n",
        "action_input -> parameters to send to the tool\n",
        "\n",
        "These are the tools you can use: {tool_names}.\n",
        "\n",
        "These are the tools descriptions:\n",
        "\n",
        "{tools}\n",
        "\n",
        "If you have enough information to answer the query use the tool \"Final Answer\". Its parameters is the solution.\n",
        "If there is not enough information, keep trying.\n",
        "\"\"\"\n",
        "\n",
        "human=\"\"\"\n",
        "Add the word \"STOP\" after each markdown snippet. Example:\n",
        "\n",
        "```json\n",
        "{{\"thought\": \"<your thoughts>\",\n",
        " \"action\": \"<tool name or Final Answer to give a final answer>\",\n",
        " \"action_input\": \"<tool parameters or the final output\"}}\n",
        "```\n",
        "STOP\n",
        "\n",
        "This is my query=\"{input}\". Write only the next step needed to solve it.\n",
        "Your answer should be based in the previous tools executions, even if you think you know the answer.\n",
        "Remember to add STOP after each snippet.\n",
        "\n",
        "These were the previous steps given to solve this query and the information you already gathered:\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        MessagesPlaceholder(\"chat_history\", optional=True),\n",
        "        (\"human\", human),\n",
        "        MessagesPlaceholder(\"agent_scratchpad\")\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "8kI0wCpDMheZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building the Agent"
      ],
      "metadata": {
        "id": "QWR8f8i4hkte"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = create_json_chat_agent(\n",
        "    tools = tools,\n",
        "    llm = llm,\n",
        "    prompt = prompt,\n",
        "    stop_sequence = [\"STOP\"],\n",
        "    template_tool_response = \"{observation}\"\n",
        ")\n",
        "\n",
        "#memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "#agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, handle_parsing_errors=True, memory=memory)\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, handle_parsing_errors=True)"
      ],
      "metadata": {
        "id": "27QqSFwLbEjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tests"
      ],
      "metadata": {
        "id": "p6DkeQtdhojI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor.invoke({\"input\": \"How much is 23 plus 17?\"})"
      ],
      "metadata": {
        "id": "HW03b22pUC5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor.invoke({\"input\": \"What is the capital of France?\"})"
      ],
      "metadata": {
        "id": "warEhFbHfMY4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}