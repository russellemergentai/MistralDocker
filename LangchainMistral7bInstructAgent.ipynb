{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/russellemergentai/MistralDocker/blob/main/LangchainMistral7bInstructAgent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iy-bo4p2ay57"
      },
      "source": [
        "T4 is fine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BS1BET3jkGWB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "0f6fe8da9aaa474599d548750c75da6e",
            "90a3de2a033b4d8f9e1c5ea05cb6cf29",
            "c2577461aeb0469c85c5d666a0f80878",
            "646adba42ca045ee9617cb58a890f8a6",
            "49394e41465c44599e059de13c576ea9",
            "5a40a9a465f04666ae1b4fca750d9b40",
            "c985837435944df9981999d077497056",
            "32de7519c908420085c337d0f506ecfe",
            "6774a0ccca3d4214a44b1a6f7bdd0583",
            "3b1ef707134943a2a06fc77d06600249",
            "b5ce2c9e04194efa967d0da8940ca02a"
          ]
        },
        "outputId": "df917ac9-0fbc-447c-c7c0-45c2cc5d6ded"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "logged in to Hugging Face\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.19)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.3)\n",
            "Requirement already satisfied: langchain-huggingface in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.35 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.40)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.6)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.38)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (3.11.13)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: numpy<2,>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.5.1+cu124)\n",
            "Requirement already satisfied: sentence-transformers>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (3.4.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (11.1.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.35->langchain) (3.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (3.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0f6fe8da9aaa474599d548750c75da6e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "#login\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "# Load the secret token\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "\n",
        "# Check if the token is set and log in\n",
        "if HF_TOKEN:\n",
        "    login(HF_TOKEN)\n",
        "    print(\"logged in to Hugging Face\")\n",
        "else:\n",
        "    print(\"Hugging Face login failed\")\n",
        "\n",
        "# Install required libraries\n",
        "!pip install langchain transformers bitsandbytes langchain-huggingface\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from langchain.agents import initialize_agent, Tool\n",
        "from langchain.agents.agent_types import AgentType\n",
        "import torch\n",
        "\n",
        "# Define BitsAndBytesConfig for 4-bit quantization\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Load the local LLM with 4-bit quantization\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "llm = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\": 0})\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side='left', add_eos_token=True, use_fast=False)\n",
        "\n",
        "# Create a text generation pipeline\n",
        "pipe = pipeline(\"text-generation\", model=llm, tokenizer=tokenizer)\n",
        "\n",
        "# Wrap the pipeline for LangChain\n",
        "common_params = {\n",
        "  'max_length': 512,\n",
        "  'eos_token_id': tokenizer.eos_token_id,\n",
        "}\n",
        "\n",
        "pipelineBase = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=llm,\n",
        "    tokenizer=tokenizer,\n",
        "    **common_params,\n",
        "    max_new_tokens=200\n",
        ")\n",
        "\n",
        "llmPipeline = HuggingFacePipeline(pipeline=pipelineBase, model_kwargs={\"temperature\": 0.1})\n",
        "\n",
        "# Define the prompt template\n",
        "prompt_template = \"\"\"\n",
        "<s>\n",
        "[INST]\n",
        "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "{query}\n",
        "[/INST]\n",
        "</s>\n",
        "[INST]Keep your response succinct.[/INST]\n",
        "\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate.from_template(prompt_template)\n",
        "\n",
        "# Define tools\n",
        "def call_llm(input_text):\n",
        "    PROMPT = PromptTemplate.from_template(prompt_template)\n",
        "    llm_chain = PROMPT | llmPipeline\n",
        "    result = llm_chain.invoke({\"query\": input_text})\n",
        "    return result\n",
        "\n",
        "local_llm = llmPipeline\n",
        "\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"Mistral\",\n",
        "        func=call_llm,\n",
        "        description=\"This tool generates a response using the local LLM pipeline and a custom prompt template.\"\n",
        "    )\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9ZfgugMBNuA8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24372c7a-2780-498e-fcb7-57bfc4fabe48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "# Create a text generation pipeline\n",
        "pipe = pipeline(\"text-generation\", model=llm, tokenizer=tokenizer)\n",
        "\n",
        "# Wrap the pipeline for LangChain\n",
        "common_params = {\n",
        "  'max_length': 512,\n",
        "  'eos_token_id': tokenizer.eos_token_id,\n",
        "}\n",
        "\n",
        "pipelineBase = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=llm,\n",
        "    tokenizer=tokenizer,\n",
        "    **common_params,\n",
        "    max_new_tokens=200\n",
        ")\n",
        "\n",
        "llmPipeline = HuggingFacePipeline(pipeline=pipelineBase, model_kwargs={\"temperature\": 0.1})\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the prompt template\n",
        "prompt_template = \"\"\"\n",
        "<s>\n",
        "   [INST]\n",
        "   Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "   If you have Final Answer, leave Action/Action Input/Observation blank.\n",
        "   {query}\n",
        "   [/INST]\n",
        "   </s>\n",
        "   [INST]\n",
        "   Use the following format:\n",
        "    Thought: [Your thought process]\n",
        "    Action: [The action to take]\n",
        "    Action Input: [Input for the action]\n",
        "    Observation: [Result of the action]\n",
        "    Final Answer: [The final answer to the query]\n",
        "   [/INST]\n",
        "\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate.from_template(prompt_template)\n",
        "\n",
        "# Define tools\n",
        "def call_llm(input_text):\n",
        "    PROMPT = PromptTemplate.from_template(prompt_template)\n",
        "    llm_chain = PROMPT | llmPipeline\n",
        "    result = llm_chain.invoke({\"query\": input_text})\n",
        "    return result\n",
        "\n",
        "local_llm = llmPipeline\n",
        "\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"Mistral\",\n",
        "        func=call_llm,\n",
        "        description=\"This tool generates a response using the local LLM pipeline and a custom prompt template.\"\n",
        "    )\n",
        "]\n",
        "\n",
        "from langchain_core.agents import AgentAction, AgentFinish\n",
        "from langchain_core.exceptions import OutputParserException\n",
        "from langchain_core.output_parsers import BaseOutputParser\n",
        "\n",
        "class CustomOutputParser(BaseOutputParser):\n",
        "    def parse(self, text: str):\n",
        "        if \"Final Answer:\" in text:\n",
        "            # Handle the final answer\n",
        "            return AgentFinish(return_values={\"output\": text.split(\"Final Answer:\")[-1].strip()}, log=text)\n",
        "        elif \"Action:\" in text and \"Action Input:\" in text:\n",
        "            # Handle action parsing\n",
        "            action = text.split(\"Action:\")[-1].split(\"Action Input:\")[0].strip()\n",
        "            action_input = text.split(\"Action Input:\")[-1].strip()\n",
        "            return AgentAction(tool=action, tool_input=action_input, log=text)\n",
        "        else:\n",
        "            # Raise an exception if the format isn't correct\n",
        "            raise OutputParserException(\"Could not parse LLM output properly.\")\n",
        "\n",
        "\n",
        "# Initialize the agent\n",
        "agent = initialize_agent(\n",
        "    tools,\n",
        "    local_llm,\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True,\n",
        "    handle_parsing_errors=True,\n",
        "    max_iterations=3,\n",
        "    output_parser=CustomOutputParser()\n",
        ")\n",
        "\n",
        "def main():\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"Enter query:\").lower()\n",
        "\n",
        "        if user_input == 0:\n",
        "            print(\"Exiting.\")\n",
        "            break\n",
        "\n",
        "        # Run the agent\n",
        "        result = agent.run(PROMPT.format(query=user_input))\n",
        "        print(result)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "z3CV9656a7p4",
        "outputId": "416edd3d-8ee9-4dd7-e5e3-a834251a76fd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter query:sort the following list into alphabetic order: D,A,B,N,K\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Both `max_new_tokens` (=200) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Both `max_new_tokens` (=200) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3mParsing LLM output produced both a final answer and a parse-able action:: Answer the following questions as best you can. You have access to the following tools:\n",
            "\n",
            "Mistral(input_text) - This tool generates a response using the local LLM pipeline and a custom prompt template.\n",
            "\n",
            "Use the following format:\n",
            "\n",
            "Question: the input question you must answer\n",
            "Thought: you should always think about what to do\n",
            "Action: the action to take, should be one of [Mistral]\n",
            "Action Input: the input to the action\n",
            "Observation: the result of the action\n",
            "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
            "Thought: I now know the final answer\n",
            "Final Answer: the final answer to the original input question\n",
            "\n",
            "Begin!\n",
            "\n",
            "Question: \n",
            "<s>\n",
            "   [INST]\n",
            "   Below is an instruction that describes a task. Write a response that appropriately completes the request. \n",
            "   If you have Final Answer, leave Action/Action Input/Observation blank.\n",
            "   sort the following list into alphabetic order: d,a,b,n,k\n",
            "   [/INST]\n",
            "   </s>\n",
            "   [INST]   \n",
            "   Use the following format:\n",
            "    Thought: [Your thought process]\n",
            "    Action: [The action to take]\n",
            "    Action Input: [Input for the action]\n",
            "    Observation: [Result of the action]\n",
            "    Final Answer: [The final answer to the query]\n",
            "   [/INST]\n",
            "\n",
            "Thought: I need to sort the given list of characters in alphabetic order.\n",
            "Action: Mistral\n",
            "Action Input: Sort the following list into alphabetic order: d,a,b,n,k\n",
            "Observation: [The result of the action will be provided by Mistral]\n",
            "Final Answer: [The final answer will be provided by Mistral]\n",
            "\n",
            "Thought: Mistral has provided the sorted list.\n",
            "Final Answer: a, d, b, k, n\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
            "Observation: Invalid or incomplete response\n",
            "Thought:"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Both `max_new_tokens` (=200) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3mParsing LLM output produced both a final answer and a parse-able action:: Answer the following questions as best you can. You have access to the following tools:\n",
            "\n",
            "Mistral(input_text) - This tool generates a response using the local LLM pipeline and a custom prompt template.\n",
            "\n",
            "Use the following format:\n",
            "\n",
            "Question: the input question you must answer\n",
            "Thought: you should always think about what to do\n",
            "Action: the action to take, should be one of [Mistral]\n",
            "Action Input: the input to the action\n",
            "Observation: the result of the action\n",
            "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
            "Thought: I now know the final answer\n",
            "Final Answer: the final answer to the original input question\n",
            "\n",
            "Begin!\n",
            "\n",
            "Question: \n",
            "<s>\n",
            "   [INST]\n",
            "   Below is an instruction that describes a task. Write a response that appropriately completes the request. \n",
            "   If you have Final Answer, leave Action/Action Input/Observation blank.\n",
            "   sort the following list into alphabetic order: d,a,b,n,k\n",
            "   [/INST]\n",
            "   </s>\n",
            "   [INST]   \n",
            "   Use the following format:\n",
            "    Thought: [Your thought process]\n",
            "    Action: [The action to take]\n",
            "    Action Input: [Input for the action]\n",
            "    Observation: [Result of the action]\n",
            "    Final Answer: [The final answer to the query]\n",
            "   [/INST]\n",
            "\n",
            "Thought:Parsing LLM output produced both a final answer and a parse-able action:: Answer the following questions as best you can. You have access to the following tools:\n",
            "\n",
            "Mistral(input_text) - This tool generates a response using the local LLM pipeline and a custom prompt template.\n",
            "\n",
            "Use the following format:\n",
            "\n",
            "Question: the input question you must answer\n",
            "Thought: you should always think about what to do\n",
            "Action: the action to take, should be one of [Mistral]\n",
            "Action Input: the input to the action\n",
            "Observation: the result of the action\n",
            "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
            "Thought: I now know the final answer\n",
            "Final Answer: the final answer to the original input question\n",
            "\n",
            "Begin!\n",
            "\n",
            "Question: \n",
            "<s>\n",
            "   [INST]\n",
            "   Below is an instruction that describes a task. Write a response that appropriately completes the request. \n",
            "   If you have Final Answer, leave Action/Action Input/Observation blank.\n",
            "   sort the following list into alphabetic order: d,a,b,n,k\n",
            "   [/INST]\n",
            "   </s>\n",
            "   [INST]   \n",
            "   Use the following format:\n",
            "    Thought: [Your thought process]\n",
            "    Action: [The action to take]\n",
            "    Action Input: [Input for the action]\n",
            "    Observation: [Result of the action]\n",
            "    Final Answer: [The final answer to the query]\n",
            "   [/INST]\n",
            "\n",
            "Thought: I need to sort the given list of characters in alphabetic order.\n",
            "Action: Mistral\n",
            "Action Input: Sort the following list into alphabetic order: d,a,b,n,k\n",
            "Observation: [The result of the action will be provided by Mistral]\n",
            "Final Answer: [The final answer will be provided by Mistral]\n",
            "\n",
            "Thought: Mistral has provided the sorted list.\n",
            "Final Answer: a, d, b, k, n\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
            "Observation: Invalid or incomplete response\n",
            "Thought:\n",
            "   Thought: The task is to sort the given list of characters in alphabetic order.\n",
            "   Action: Mistral\n",
            "   Action Input: Sort the following list into alphabetic order: d,a,b,n,k\n",
            "   Observation: The sorted list is: a, b, d, k, n\n",
            "   Final Answer: The sorted list is: a, b, d, k, n\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
            "Observation: Invalid or incomplete response\n",
            "Thought:\u001b[32;1m\u001b[1;3mParsing LLM output produced both a final answer and a parse-able action:: Answer the following questions as best you can. You have access to the following tools:\n",
            "\n",
            "Mistral(input_text) - This tool generates a response using the local LLM pipeline and a custom prompt template.\n",
            "\n",
            "Use the following format:\n",
            "\n",
            "Question: the input question you must answer\n",
            "Thought: you should always think about what to do\n",
            "Action: the action to take, should be one of [Mistral]\n",
            "Action Input: the input to the action\n",
            "Observation: the result of the action\n",
            "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
            "Thought: I now know the final answer\n",
            "Final Answer: the final answer to the original input question\n",
            "\n",
            "Begin!\n",
            "\n",
            "Question: \n",
            "<s>\n",
            "   [INST]\n",
            "   Below is an instruction that describes a task. Write a response that appropriately completes the request. \n",
            "   If you have Final Answer, leave Action/Action Input/Observation blank.\n",
            "   sort the following list into alphabetic order: d,a,b,n,k\n",
            "   [/INST]\n",
            "   </s>\n",
            "   [INST]   \n",
            "   Use the following format:\n",
            "    Thought: [Your thought process]\n",
            "    Action: [The action to take]\n",
            "    Action Input: [Input for the action]\n",
            "    Observation: [Result of the action]\n",
            "    Final Answer: [The final answer to the query]\n",
            "   [/INST]\n",
            "\n",
            "Thought:Parsing LLM output produced both a final answer and a parse-able action:: Answer the following questions as best you can. You have access to the following tools:\n",
            "\n",
            "Mistral(input_text) - This tool generates a response using the local LLM pipeline and a custom prompt template.\n",
            "\n",
            "Use the following format:\n",
            "\n",
            "Question: the input question you must answer\n",
            "Thought: you should always think about what to do\n",
            "Action: the action to take, should be one of [Mistral]\n",
            "Action Input: the input to the action\n",
            "Observation: the result of the action\n",
            "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
            "Thought: I now know the final answer\n",
            "Final Answer: the final answer to the original input question\n",
            "\n",
            "Begin!\n",
            "\n",
            "Question: \n",
            "<s>\n",
            "   [INST]\n",
            "   Below is an instruction that describes a task. Write a response that appropriately completes the request. \n",
            "   If you have Final Answer, leave Action/Action Input/Observation blank.\n",
            "   sort the following list into alphabetic order: d,a,b,n,k\n",
            "   [/INST]\n",
            "   </s>\n",
            "   [INST]   \n",
            "   Use the following format:\n",
            "    Thought: [Your thought process]\n",
            "    Action: [The action to take]\n",
            "    Action Input: [Input for the action]\n",
            "    Observation: [Result of the action]\n",
            "    Final Answer: [The final answer to the query]\n",
            "   [/INST]\n",
            "\n",
            "Thought: I need to sort the given list of characters in alphabetic order.\n",
            "Action: Mistral\n",
            "Action Input: Sort the following list into alphabetic order: d,a,b,n,k\n",
            "Observation: [The result of the action will be provided by Mistral]\n",
            "Final Answer: [The final answer will be provided by Mistral]\n",
            "\n",
            "Thought: Mistral has provided the sorted list.\n",
            "Final Answer: a, d, b, k, n\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
            "Observation: Invalid or incomplete response\n",
            "Thought:Parsing LLM output produced both a final answer and a parse-able action:: Answer the following questions as best you can. You have access to the following tools:\n",
            "\n",
            "Mistral(input_text) - This tool generates a response using the local LLM pipeline and a custom prompt template.\n",
            "\n",
            "Use the following format:\n",
            "\n",
            "Question: the input question you must answer\n",
            "Thought: you should always think about what to do\n",
            "Action: the action to take, should be one of [Mistral]\n",
            "Action Input: the input to the action\n",
            "Observation: the result of the action\n",
            "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
            "Thought: I now know the final answer\n",
            "Final Answer: the final answer to the original input question\n",
            "\n",
            "Begin!\n",
            "\n",
            "Question: \n",
            "<s>\n",
            "   [INST]\n",
            "   Below is an instruction that describes a task. Write a response that appropriately completes the request. \n",
            "   If you have Final Answer, leave Action/Action Input/Observation blank.\n",
            "   sort the following list into alphabetic order: d,a,b,n,k\n",
            "   [/INST]\n",
            "   </s>\n",
            "   [INST]   \n",
            "   Use the following format:\n",
            "    Thought: [Your thought process]\n",
            "    Action: [The action to take]\n",
            "    Action Input: [Input for the action]\n",
            "    Observation: [Result of the action]\n",
            "    Final Answer: [The final answer to the query]\n",
            "   [/INST]\n",
            "\n",
            "Thought:Parsing LLM output produced both a final answer and a parse-able action:: Answer the following questions as best you can. You have access to the following tools:\n",
            "\n",
            "Mistral(input_text) - This tool generates a response using the local LLM pipeline and a custom prompt template.\n",
            "\n",
            "Use the following format:\n",
            "\n",
            "Question: the input question you must answer\n",
            "Thought: you should always think about what to do\n",
            "Action: the action to take, should be one of [Mistral]\n",
            "Action Input: the input to the action\n",
            "Observation: the result of the action\n",
            "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
            "Thought: I now know the final answer\n",
            "Final Answer: the final answer to the original input question\n",
            "\n",
            "Begin!\n",
            "\n",
            "Question: \n",
            "<s>\n",
            "   [INST]\n",
            "   Below is an instruction that describes a task. Write a response that appropriately completes the request. \n",
            "   If you have Final Answer, leave Action/Action Input/Observation blank.\n",
            "   sort the following list into alphabetic order: d,a,b,n,k\n",
            "   [/INST]\n",
            "   </s>\n",
            "   [INST]   \n",
            "   Use the following format:\n",
            "    Thought: [Your thought process]\n",
            "    Action: [The action to take]\n",
            "    Action Input: [Input for the action]\n",
            "    Observation: [Result of the action]\n",
            "    Final Answer: [The final answer to the query]\n",
            "   [/INST]\n",
            "\n",
            "Thought: I need to sort the given list of characters in alphabetic order.\n",
            "Action: Mistral\n",
            "Action Input: Sort the following list into alphabetic order: d,a,b,n,k\n",
            "Observation: [The result of the action will be provided by Mistral]\n",
            "Final Answer: [The final answer will be provided by Mistral]\n",
            "\n",
            "Thought: Mistral has provided the sorted list.\n",
            "Final Answer: a, d, b, k, n\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
            "Observation: Invalid or incomplete response\n",
            "Thought:\n",
            "   Thought: The task is to sort the given list of characters in alphabetic order.\n",
            "   Action: Mistral\n",
            "   Action Input: Sort the following list into alphabetic order: d,a,b,n,k\n",
            "   Observation: The sorted list is: a, b, d, k, n\n",
            "   Final Answer: The sorted list is: a, b, d, k, n\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
            "Observation: Invalid or incomplete response\n",
            "Thought:\n",
            "   Thought: The task is to sort the given list of characters in alphabetic order.\n",
            "   Action: Mistral\n",
            "   Action Input: Sort the following list into alphabetic order: d,a,b,n,k\n",
            "   Observation: The sorted list is: a, b, d, k, n\n",
            "   Final Answer: The sorted list is: a, b, d, k, n\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
            "Observation: Invalid or incomplete response\n",
            "Thought:\u001b[32;1m\u001b[1;3m\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Agent stopped due to iteration limit or time limit.\n",
            "Enter query:sort the following numbers into ascending values: 9,4,2,6,7,4,1,5,6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Both `max_new_tokens` (=200) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Both `max_new_tokens` (=200) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3mParsing LLM output produced both a final answer and a parse-able action:: Answer the following questions as best you can. You have access to the following tools:\n",
            "\n",
            "Mistral(input_text) - This tool generates a response using the local LLM pipeline and a custom prompt template.\n",
            "\n",
            "Use the following format:\n",
            "\n",
            "Question: the input question you must answer\n",
            "Thought: you should always think about what to do\n",
            "Action: the action to take, should be one of [Mistral]\n",
            "Action Input: the input to the action\n",
            "Observation: the result of the action\n",
            "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
            "Thought: I now know the final answer\n",
            "Final Answer: the final answer to the original input question\n",
            "\n",
            "Begin!\n",
            "\n",
            "Question: \n",
            "<s>\n",
            "   [INST]\n",
            "   Below is an instruction that describes a task. Write a response that appropriately completes the request. \n",
            "   If you have Final Answer, leave Action/Action Input/Observation blank.\n",
            "   sort the following numbers into ascending values: 9,4,2,6,7,4,1,5,6\n",
            "   [/INST]\n",
            "   </s>\n",
            "   [INST]   \n",
            "   Use the following format:\n",
            "    Thought: [Your thought process]\n",
            "    Action: [The action to take]\n",
            "    Action Input: [Input for the action]\n",
            "    Observation: [Result of the action]\n",
            "    Final Answer: [The final answer to the query]\n",
            "   [/INST]\n",
            "\n",
            "Thought: I need to sort the given numbers in ascending order.\n",
            "Action: Sort the numbers\n",
            "Action Input: 9,4,2,6,7,4,1,5,6\n",
            "Observation: [9, 4, 2, 4, 1, 5, 6, 6, 7]\n",
            "Final Answer: [1, 2, 4, 4, 5, 6, 6, 7, 9]\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
            "Observation: Invalid or incomplete response\n",
            "Thought:"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Both `max_new_tokens` (=200) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3mParsing LLM output produced both a final answer and a parse-able action:: Answer the following questions as best you can. You have access to the following tools:\n",
            "\n",
            "Mistral(input_text) - This tool generates a response using the local LLM pipeline and a custom prompt template.\n",
            "\n",
            "Use the following format:\n",
            "\n",
            "Question: the input question you must answer\n",
            "Thought: you should always think about what to do\n",
            "Action: the action to take, should be one of [Mistral]\n",
            "Action Input: the input to the action\n",
            "Observation: the result of the action\n",
            "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
            "Thought: I now know the final answer\n",
            "Final Answer: the final answer to the original input question\n",
            "\n",
            "Begin!\n",
            "\n",
            "Question: \n",
            "<s>\n",
            "   [INST]\n",
            "   Below is an instruction that describes a task. Write a response that appropriately completes the request. \n",
            "   If you have Final Answer, leave Action/Action Input/Observation blank.\n",
            "   sort the following numbers into ascending values: 9,4,2,6,7,4,1,5,6\n",
            "   [/INST]\n",
            "   </s>\n",
            "   [INST]   \n",
            "   Use the following format:\n",
            "    Thought: [Your thought process]\n",
            "    Action: [The action to take]\n",
            "    Action Input: [Input for the action]\n",
            "    Observation: [Result of the action]\n",
            "    Final Answer: [The final answer to the query]\n",
            "   [/INST]\n",
            "\n",
            "Thought:Parsing LLM output produced both a final answer and a parse-able action:: Answer the following questions as best you can. You have access to the following tools:\n",
            "\n",
            "Mistral(input_text) - This tool generates a response using the local LLM pipeline and a custom prompt template.\n",
            "\n",
            "Use the following format:\n",
            "\n",
            "Question: the input question you must answer\n",
            "Thought: you should always think about what to do\n",
            "Action: the action to take, should be one of [Mistral]\n",
            "Action Input: the input to the action\n",
            "Observation: the result of the action\n",
            "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
            "Thought: I now know the final answer\n",
            "Final Answer: the final answer to the original input question\n",
            "\n",
            "Begin!\n",
            "\n",
            "Question: \n",
            "<s>\n",
            "   [INST]\n",
            "   Below is an instruction that describes a task. Write a response that appropriately completes the request. \n",
            "   If you have Final Answer, leave Action/Action Input/Observation blank.\n",
            "   sort the following numbers into ascending values: 9,4,2,6,7,4,1,5,6\n",
            "   [/INST]\n",
            "   </s>\n",
            "   [INST]   \n",
            "   Use the following format:\n",
            "    Thought: [Your thought process]\n",
            "    Action: [The action to take]\n",
            "    Action Input: [Input for the action]\n",
            "    Observation: [Result of the action]\n",
            "    Final Answer: [The final answer to the query]\n",
            "   [/INST]\n",
            "\n",
            "Thought: I need to sort the given numbers in ascending order.\n",
            "Action: Sort the numbers\n",
            "Action Input: 9,4,2,6,7,4,1,5,6\n",
            "Observation: [9, 4, 2, 4, 1, 5, 6, 6, 7]\n",
            "Final Answer: [1, 2, 4, 4, 5, 6, 6, 7, 9]\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
            "Observation: Invalid or incomplete response\n",
            "Thought:\n",
            "   Thought: The task is to sort the given numbers in ascending order.\n",
            "   Action: Sorting the numbers\n",
            "   Action Input: 9,4,2,6,7,4,1,5,6\n",
            "   Observation: 1, 2, 4 (twice), 4, 5, 6 (twice), 7, 9\n",
            "   Final Answer: 1, 2, 4, 4, 5, 6, 6, 7, 9\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
            "Observation: Invalid or incomplete response\n",
            "Thought:\u001b[32;1m\u001b[1;3mParsing LLM output produced both a final answer and a parse-able action:: Answer the following questions as best you can. You have access to the following tools:\n",
            "\n",
            "Mistral(input_text) - This tool generates a response using the local LLM pipeline and a custom prompt template.\n",
            "\n",
            "Use the following format:\n",
            "\n",
            "Question: the input question you must answer\n",
            "Thought: you should always think about what to do\n",
            "Action: the action to take, should be one of [Mistral]\n",
            "Action Input: the input to the action\n",
            "Observation: the result of the action\n",
            "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
            "Thought: I now know the final answer\n",
            "Final Answer: the final answer to the original input question\n",
            "\n",
            "Begin!\n",
            "\n",
            "Question: \n",
            "<s>\n",
            "   [INST]\n",
            "   Below is an instruction that describes a task. Write a response that appropriately completes the request. \n",
            "   If you have Final Answer, leave Action/Action Input/Observation blank.\n",
            "   sort the following numbers into ascending values: 9,4,2,6,7,4,1,5,6\n",
            "   [/INST]\n",
            "   </s>\n",
            "   [INST]   \n",
            "   Use the following format:\n",
            "    Thought: [Your thought process]\n",
            "    Action: [The action to take]\n",
            "    Action Input: [Input for the action]\n",
            "    Observation: [Result of the action]\n",
            "    Final Answer: [The final answer to the query]\n",
            "   [/INST]\n",
            "\n",
            "Thought:Parsing LLM output produced both a final answer and a parse-able action:: Answer the following questions as best you can. You have access to the following tools:\n",
            "\n",
            "Mistral(input_text) - This tool generates a response using the local LLM pipeline and a custom prompt template.\n",
            "\n",
            "Use the following format:\n",
            "\n",
            "Question: the input question you must answer\n",
            "Thought: you should always think about what to do\n",
            "Action: the action to take, should be one of [Mistral]\n",
            "Action Input: the input to the action\n",
            "Observation: the result of the action\n",
            "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
            "Thought: I now know the final answer\n",
            "Final Answer: the final answer to the original input question\n",
            "\n",
            "Begin!\n",
            "\n",
            "Question: \n",
            "<s>\n",
            "   [INST]\n",
            "   Below is an instruction that describes a task. Write a response that appropriately completes the request. \n",
            "   If you have Final Answer, leave Action/Action Input/Observation blank.\n",
            "   sort the following numbers into ascending values: 9,4,2,6,7,4,1,5,6\n",
            "   [/INST]\n",
            "   </s>\n",
            "   [INST]   \n",
            "   Use the following format:\n",
            "    Thought: [Your thought process]\n",
            "    Action: [The action to take]\n",
            "    Action Input: [Input for the action]\n",
            "    Observation: [Result of the action]\n",
            "    Final Answer: [The final answer to the query]\n",
            "   [/INST]\n",
            "\n",
            "Thought: I need to sort the given numbers in ascending order.\n",
            "Action: Sort the numbers\n",
            "Action Input: 9,4,2,6,7,4,1,5,6\n",
            "Observation: [9, 4, 2, 4, 1, 5, 6, 6, 7]\n",
            "Final Answer: [1, 2, 4, 4, 5, 6, 6, 7, 9]\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
            "Observation: Invalid or incomplete response\n",
            "Thought:Parsing LLM output produced both a final answer and a parse-able action:: Answer the following questions as best you can. You have access to the following tools:\n",
            "\n",
            "Mistral(input_text) - This tool generates a response using the local LLM pipeline and a custom prompt template.\n",
            "\n",
            "Use the following format:\n",
            "\n",
            "Question: the input question you must answer\n",
            "Thought: you should always think about what to do\n",
            "Action: the action to take, should be one of [Mistral]\n",
            "Action Input: the input to the action\n",
            "Observation: the result of the action\n",
            "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
            "Thought: I now know the final answer\n",
            "Final Answer: the final answer to the original input question\n",
            "\n",
            "Begin!\n",
            "\n",
            "Question: \n",
            "<s>\n",
            "   [INST]\n",
            "   Below is an instruction that describes a task. Write a response that appropriately completes the request. \n",
            "   If you have Final Answer, leave Action/Action Input/Observation blank.\n",
            "   sort the following numbers into ascending values: 9,4,2,6,7,4,1,5,6\n",
            "   [/INST]\n",
            "   </s>\n",
            "   [INST]   \n",
            "   Use the following format:\n",
            "    Thought: [Your thought process]\n",
            "    Action: [The action to take]\n",
            "    Action Input: [Input for the action]\n",
            "    Observation: [Result of the action]\n",
            "    Final Answer: [The final answer to the query]\n",
            "   [/INST]\n",
            "\n",
            "Thought:Parsing LLM output produced both a final answer and a parse-able action:: Answer the following questions as best you can. You have access to the following tools:\n",
            "\n",
            "Mistral(input_text) - This tool generates a response using the local LLM pipeline and a custom prompt template.\n",
            "\n",
            "Use the following format:\n",
            "\n",
            "Question: the input question you must answer\n",
            "Thought: you should always think about what to do\n",
            "Action: the action to take, should be one of [Mistral]\n",
            "Action Input: the input to the action\n",
            "Observation: the result of the action\n",
            "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
            "Thought: I now know the final answer\n",
            "Final Answer: the final answer to the original input question\n",
            "\n",
            "Begin!\n",
            "\n",
            "Question: \n",
            "<s>\n",
            "   [INST]\n",
            "   Below is an instruction that describes a task. Write a response that appropriately completes the request. \n",
            "   If you have Final Answer, leave Action/Action Input/Observation blank.\n",
            "   sort the following numbers into ascending values: 9,4,2,6,7,4,1,5,6\n",
            "   [/INST]\n",
            "   </s>\n",
            "   [INST]   \n",
            "   Use the following format:\n",
            "    Thought: [Your thought process]\n",
            "    Action: [The action to take]\n",
            "    Action Input: [Input for the action]\n",
            "    Observation: [Result of the action]\n",
            "    Final Answer: [The final answer to the query]\n",
            "   [/INST]\n",
            "\n",
            "Thought: I need to sort the given numbers in ascending order.\n",
            "Action: Sort the numbers\n",
            "Action Input: 9,4,2,6,7,4,1,5,6\n",
            "Observation: [9, 4, 2, 4, 1, 5, 6, 6, 7]\n",
            "Final Answer: [1, 2, 4, 4, 5, 6, 6, 7, 9]\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
            "Observation: Invalid or incomplete response\n",
            "Thought:\n",
            "   Thought: The task is to sort the given numbers in ascending order.\n",
            "   Action: Sorting the numbers\n",
            "   Action Input: 9,4,2,6,7,4,1,5,6\n",
            "   Observation: 1, 2, 4 (twice), 4, 5, 6 (twice), 7, 9\n",
            "   Final Answer: 1, 2, 4, 4, 5, 6, 6, 7, 9\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
            "Observation: Invalid or incomplete response\n",
            "Thought:\n",
            "   Thought: The task is to sort the given numbers in ascending order.\n",
            "   Action: Sorting the numbers\n",
            "   Action Input: 9,4,2,6,7,4,1,5,6\n",
            "   Observation: 1, 2, 4 (twice), 4, 5, 6 (twice), 7, 9\n",
            "   Final Answer: 1, 2, 4, 4, 5, 6, 6, 7, 9\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
            "Observation: Invalid or incomplete response\n",
            "Thought:\u001b[32;1m\u001b[1;3m\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Agent stopped due to iteration limit or time limit.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-b7182e26a968>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-b7182e26a968>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter query:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m# Run the agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "1wIm9Z4duPaleS7bGc6fdTT8-mONa87RF",
      "authorship_tag": "ABX9TyPNEzKqfAf7KsbBSdXK4+YB",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0f6fe8da9aaa474599d548750c75da6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_90a3de2a033b4d8f9e1c5ea05cb6cf29",
              "IPY_MODEL_c2577461aeb0469c85c5d666a0f80878",
              "IPY_MODEL_646adba42ca045ee9617cb58a890f8a6"
            ],
            "layout": "IPY_MODEL_49394e41465c44599e059de13c576ea9"
          }
        },
        "90a3de2a033b4d8f9e1c5ea05cb6cf29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a40a9a465f04666ae1b4fca750d9b40",
            "placeholder": "​",
            "style": "IPY_MODEL_c985837435944df9981999d077497056",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "c2577461aeb0469c85c5d666a0f80878": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32de7519c908420085c337d0f506ecfe",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6774a0ccca3d4214a44b1a6f7bdd0583",
            "value": 3
          }
        },
        "646adba42ca045ee9617cb58a890f8a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b1ef707134943a2a06fc77d06600249",
            "placeholder": "​",
            "style": "IPY_MODEL_b5ce2c9e04194efa967d0da8940ca02a",
            "value": " 3/3 [01:26&lt;00:00, 28.24s/it]"
          }
        },
        "49394e41465c44599e059de13c576ea9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a40a9a465f04666ae1b4fca750d9b40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c985837435944df9981999d077497056": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "32de7519c908420085c337d0f506ecfe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6774a0ccca3d4214a44b1a6f7bdd0583": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3b1ef707134943a2a06fc77d06600249": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5ce2c9e04194efa967d0da8940ca02a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}